# Задание 1: Эксперименты с глубиной сети

## 1.1 Сравнение моделей разной глубины

Создайл и обучил модели с требуемым количеством слоев:
- 1 слой (линейный классификатор)
- 2 слоя (1 скрытый)
- 3 слоя (2 скрытых)
- 5 слоев (4 скрытых)
- 7 слоев (6 скрытых)

Обучил все модели на датасетах mnist и cifar, для всех моделей построил графики обучения. Они доступны в проекте: plots/depth_experiments

На датасете mnist точность на test и train отличается незначительно

На датасете cifar точность на train сильно превышает test

График обучения mnist для модели 7 слоев (6 скрытых)
![7_layers_mnist_base](https://github.com/user-attachments/assets/0d586c43-abe6-4c29-a239-3880297eab32)

График обучения cifar для модели 7 слоев (6 скрытых)
![7_layers_cifar_base](https://github.com/user-attachments/assets/071c04a1-2764-4063-b794-61d77bd7d8ea)

Для первых трех моделей время обучения составило около 10 - 12 секунд на эпоху (обучал на gpu)

Для последних двух моделей время обучения возросло, около 15 - 18 секунд на эпоху

## 1.2 Анализ переобучения
Из графиков стало понятно, что обучение происходит нестабильно (сильно скачет loss), а также было было переобучение (особенно на cifar)

Я обучил те же самые модели, но с добавлением Dropout и BatchNorm, построил графики и добавил их туда же: plots/depth_experiments

loss перестал "дергаться" и начал плавно снижаться, а качество на test и train стало отличаться менее значительно. Переобучения удалось избажать

График обучения mnist для модели 7 слоев (6 скрытых) с добавлением Dropout и BatchNorm
![7_layers_mnist_with_bn_do](https://github.com/user-attachments/assets/98de8d9f-99e1-4e6e-b43b-d6988673d166)

График обучения cifar для модели 7 слоев (6 скрытых) с добавлением Dropout и BatchNorm
![7_layers_cifar_with_bn_do](https://github.com/user-attachments/assets/e5cdc48f-7a20-42ba-8532-99aa302479a0)

Я составил csv таблицы с результатами обучения. Они доступны: results/depth_experiments

Согласно этим результатам, для обоих датасетов лучшая глубина - это 5 слоев (4 скрытых)

# Задание 2: Эксперименты с шириной сети

## 2.1 Сравнение моделей разной ширины

Создал модели с различной шириной слоев:
- Узкие слои: [64, 32, 16]
- Средние слои: [256, 128, 64]
- Широкие слои: [1024, 512, 256]
- Очень широкие слои: [2048, 1024, 512]

Обучил модели и составил таблицу с результатами: results/width_experiments

Лучший результат достигался на широких слоях (для всех датасетов)

Количество параметров возрастает вместе с увеличением ширины

## 2.2 Оптимизация архитектуры

Использовал grid search для поиска лучшей комбинации.
Лучше всего себя показала убывающая последовательность (сужение): [512, 256, 128]

Приведу heatmap для mnist

![heatmap_mnist](https://github.com/user-attachments/assets/a3d25956-6b91-4a3c-b2c5-209616d2ecaf)

И для cifar

![heatmap_cifar](https://github.com/user-attachments/assets/26b0d92d-e5e0-4ddf-bfb9-e5a7314c9874)

# Задание 3: Эксперименты с регуляризацией

## 3.1 Сравнение техник регуляризации

Если регуляризация слишком слабая (или отсутствует), то обучение становится нестабильным и loss сильно колеблется
Пример:

mnist без регуляризации
![no_reg_mnist_history](https://github.com/user-attachments/assets/f97ab101-7fb6-4508-9d01-07b4242064c2)

dropout + batch norm
![bn_dropout_mnist_history](https://github.com/user-attachments/assets/d7b9e113-305f-4567-9ba0-0efc700a4bb3)

Сильно меняется распределение весов. Распределение без регуляризации:

![no_reg_mnist_weights](https://github.com/user-attachments/assets/2541c8f3-2727-499f-bc47-22852127600d)

Распределение с dropout и batch norm

![bn_dropout_mnist_weights](https://github.com/user-attachments/assets/44d69b7b-4de9-4357-9740-0aadf88edb20)


